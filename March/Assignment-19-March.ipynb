{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70edfd60-5720-43d4-8c79-641ef178b84c",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate itsapplication.\n",
    "\n",
    "\n",
    "Min-Max scaling is a data preprocessing technique used to transform numerical features in a dataset into a specific range, typically between 0 and 1. This technique is helpful when features have varying scales and ranges, as it helps to normalize the data and bring all features to a similar scale\n",
    "\n",
    "Here's how Min-Max scaling is used in data preprocessing:\n",
    "\n",
    "1. **Normalization:** Min-Max scaling normalizes the features. It brings all the features to a common scale, which can help prevent features with larger values from dominating the analysis or model training process.\n",
    "\n",
    "2. **Feature Comparison:** It allows for better comparison between features. When the features are on the same scale, it becomes easier to understand their relative importance and relationships.\n",
    "\n",
    "3. **Algorithm Sensitivity:** Many machine learning algorithms are sensitive to the scale of input features. Features with larger scales might influence the algorithm more than features with smaller scales. Min-Max scaling can mitigate this issue.\n",
    "\n",
    "4. **Visualization:** Scaled features are more suitable for visualization. Plots and graphs become easier to interpret when the features are within a similar range.\n",
    "\n",
    "\n",
    "Let's consider a practical example to illustrate the application of Min-Max scaling in data preprocessing.\n",
    "\n",
    "**Example: House Price Prediction**\n",
    "\n",
    "Suppose you're working on a machine learning project to predict house prices based on various features such as square footage, number of bedrooms, and neighborhood. The dataset contains the following features:\n",
    "\n",
    "* Square footage (ranging from 800 to 4000)\n",
    "* Number of bedrooms (ranging from 1 to 5)\n",
    "* Neighborhood popularity (ranging from 1 to 10)\n",
    "\n",
    "We will use Min-Max scaling to preprocess the data before training a regression model.\n",
    "\n",
    "These scaled values can now be used as input features for training your machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723364f9-f9a5-45a0-bf17-fbacb5709c11",
   "metadata": {},
   "source": [
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?  Provide an example to illustrate its application.\n",
    "\n",
    "\n",
    "The Unit Vector technique, also known as Normalization or L2 normalization, is another method used for feature scaling in data preprocessing.\n",
    "\n",
    "Unlike Min-Max scaling that scales features to a specific range (usually [0, 1]), the Unit Vector technique scales each feature to have a Euclidean norm (magnitude) of 1. This ensures that the transformed feature vector becomes a unit vector in a high-dimensional space.\n",
    "\n",
    "The main difference between Min-Max scaling and the Unit Vector technique is that Min-Max scaling adjusts the range of values, while the Unit Vector technique adjusts the scale and direction of the feature vectors.\n",
    "\n",
    "Let's consider an example to illustrate the application of the Unit Vector technique:\n",
    "\n",
    "Suppose you have a dataset of two-dimensional vectors representing user preferences for movies. Each vector has two features: the user's rating for action movies and their rating for comedy movies.\n",
    "\n",
    "Here, we can normalize each use vector to have a magnitude of 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29ac7f0-779d-4d8a-b4de-872b1f4a3a15",
   "metadata": {},
   "source": [
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "Principal Component Analysis (PCA) is a widely used technique in machine learning and data analysis for dimensionality reduction. **It helps in transforming high-dimensional data into a lower-dimensional representation while preserving the most important information or variance in the data**. PCA achieves this by identifying the principal components, which are linear combinations of the original features, in such a way that the first principal component captures the most variance, the second captures the second most, and so on.\n",
    "\n",
    "The steps involved in PCA are as follows:\n",
    "\n",
    "* Standardize the Data\n",
    "* Compute Covariance Matrix\n",
    "* Calculate Eigenvectors and Eigenvalues\n",
    "* Select Principal Components\n",
    "* Project Data onto New Space\n",
    "\n",
    "Let's illustrate PCA's application with an example involving a dataset of two-dimensional points:\n",
    "\n",
    "Suppose you have a dataset of points in a 2D space:\n",
    "\n",
    "Original data points:\n",
    "\n",
    "Point 1: [2, 3]\n",
    "Point 2: [4, 5]\n",
    "Point 3: [6, 7]\n",
    "\n",
    "Then the dataset can been effectively reduced to a 1D representation using PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433dd7cf-a8fd-4540-898f-fe77576ce747",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "PCA (Principal Component Analysis) is closely related to feature extraction, as it can be used as a technique for extracting new features from existing ones. In feature extraction, the goal is to transform the original features into a new set of features that captures the most important information while reducing the dimensionality of the data. PCA achieves this by projecting the data onto a new subspace defined by the principal components, effectively creating a set of derived features that are linear combinations of the original features.\n",
    "\n",
    "Here's how PCA can be used for feature extraction:\n",
    "\n",
    "* Compute Eigenvectors and Eigenvalues: In the PCA process, you calculate the eigenvectors and eigenvalues of the covariance matrix of the original data.\n",
    "\n",
    "* Select Principal Components: You select the top k eigenvectors corresponding to the highest eigenvalues. These eigenvectors represent the principal components, which capture the directions of highest variance in the data.\n",
    "\n",
    "* Project Data: You project the original data onto the subspace defined by the selected principal components. This transforms the data into a new set of features, which are linear combinations of the original features.\n",
    "\n",
    "* New Feature Space: The new features are effectively the coordinates of the data points in the reduced-dimensional space defined by the principal components.\n",
    "\n",
    "Example: PCA for Feature Extraction\n",
    "\n",
    "Let's illustrate PCA's use for feature extraction with a simple example involving images. Suppose you have grayscale images of handwritten digits (e.g., digits 0 to 9) that are represented as pixel values in a 28x28 grid. Each pixel serves as a feature, making a total of 784 features per image.\n",
    "\n",
    "However, you want to reduce the dimensionality for computational efficiency or to avoid overfitting in a machine learning model. You can use PCA for feature extraction in this scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f014b75f-929d-4a45-b237-465c52dcc391",
   "metadata": {},
   "source": [
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "\n",
    "we can use libraries such as scikit-learn to perform Min-Max scaling of the required features\n",
    "\n",
    "I will scale all the three features(price, rating, and delivery time) within the range of 0 to 1.\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit_transform(df['price', 'rating', 'delivery time'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4097572-faf9-48a1-9b85-7a5927c37375",
   "metadata": {},
   "source": [
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "Here's how I would use PCA for dimensionality reduction in the stock price prediction project:\n",
    "\n",
    "**Step 1: Understand the Dataset**\n",
    "\n",
    "First, make sure you have a clear understanding of the features in your dataset. These features might include various financial indicators, market trends, company-specific data, and more.\n",
    "\n",
    "**Step 2: Data Preprocessing**\n",
    "\n",
    "Before applying PCA, ensure that your data is properly preprocessed. This includes handling missing values, scaling features if they are on different scales, and any other necessary data cleaning steps.\n",
    "\n",
    "**Step 3: Standardize the Data**\n",
    "\n",
    "It's important to standardize the data to ensure that all features have zero mean and unit variance. This step is crucial for PCA to work effectively, as features with larger variances might dominate the principal components.\n",
    "\n",
    "**Step 4: Apply PCA**\n",
    "\n",
    "1. **Calculate Covariance Matrix:** Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "2. **Calculate Eigenvectors and Eigenvalues:** Compute the eigenvectors and eigenvalues of the covariance matrix. These eigenvectors represent the directions of maximum variance in the original data.\n",
    "\n",
    "3. **Select Principal Components:** Sort the eigenvalues in decreasing order and choose the top \\(k\\) eigenvectors corresponding to the highest eigenvalues. The value of \\(k\\) is determined by how many principal components you want to retain in the reduced-dimensional space.\n",
    "\n",
    "4. **Project Data:** Project the original data onto the subspace spanned by the selected \\(k\\) principal components. This reduces the dimensionality of the data from the original number of features to \\(k\\) principal components.\n",
    "\n",
    "**Step 5: Interpret Results**\n",
    "\n",
    "After applying PCA, you'll have a reduced-dimensional dataset that retains most of the variance present in the original data. Each principal component is a linear combination of the original features.\n",
    "\n",
    "**Step 6: Build the Prediction Model**\n",
    "\n",
    "we can now use the reduced-dimensional dataset for training your stock price prediction model. The reduced dataset can lead to faster training times, improved model generalization, and potentially better performance by removing noise and focusing on the most important dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521f23a7-b35e-40a9-9d18-a46d58b3a757",
   "metadata": {},
   "source": [
    "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "253ed8b1-9616-4bd5-b244-83bebffdbdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.        ]\n",
      " [-0.57894737]\n",
      " [-0.05263158]\n",
      " [ 0.47368421]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "data =  [1, 5, 10, 15, 20]\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "data_reshaped = [[x] for x in data]\n",
    "\n",
    "data_min_max = scaler.fit_transform(data_reshaped)\n",
    "print(data_min_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ac07b7-7b46-4d19-bff7-eb3f8d0c684c",
   "metadata": {},
   "source": [
    "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d68e427e-894a-4612-8852-dc55b5d7c9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principal Components:\n",
      "[[ 9.20952189e-02 -8.86790101e-01  7.28571731e-01  5.66933994e-04\n",
      "   1.30583334e-16]\n",
      " [-1.17625269e+00  1.65441774e+00  1.41627071e-01  1.65954780e-01\n",
      "   1.30583334e-16]\n",
      " [ 6.57204726e-01 -8.18897666e-01 -4.17413472e-01  3.36187442e-01\n",
      "   1.30583334e-16]\n",
      " [ 2.92793015e+00  4.96249203e-01 -1.39731339e-01 -2.36337727e-01\n",
      "   1.30583334e-16]\n",
      " [-2.50097740e+00 -4.44979177e-01 -3.13053991e-01 -2.66371429e-01\n",
      "   1.30583334e-16]]\n",
      "\n",
      "Explained Variance Ratio:\n",
      "[7.43376469e-01 2.07069548e-01 3.76177075e-02 1.19362753e-02\n",
      " 3.80625158e-33]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Creating a dataset of our own\n",
    "data = np.array([\n",
    "    [\"Male\", 170, 65, 25, 120],\n",
    "    [\"Female\", 160, 55, 30, 130],\n",
    "    [\"Male\", 175, 70, 28, 125],\n",
    "    [\"Female\", 180, 75, 35, 115],\n",
    "    [\"Male\", 165, 50, 22, 135]\n",
    "])\n",
    "\n",
    "# Separate categorical and numerical columns\n",
    "categorical_columns = [0]  # Column index of \"gender\"\n",
    "numerical_columns = [1, 2, 3, 4]\n",
    "\n",
    "# Define column transformer for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"categorical\", OneHotEncoder(), categorical_columns),\n",
    "        (\"numerical\", StandardScaler(), numerical_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply preprocessing\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "principal_components = pca.fit_transform(data_preprocessed)\n",
    "\n",
    "# Calculating explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "print(\"Principal Components:\")\n",
    "print(principal_components)\n",
    "print(\"\\nExplained Variance Ratio:\")\n",
    "print(explained_variance_ratio)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "005c4c9c-b49a-432c-9fb7-27b61f450cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principal Components:\n",
      "[[ 9.20952189e-02 -8.86790101e-01  7.28571731e-01  5.66933994e-04\n",
      "   1.30583334e-16]\n",
      " [-1.17625269e+00  1.65441774e+00  1.41627071e-01  1.65954780e-01\n",
      "   1.30583334e-16]\n",
      " [ 6.57204726e-01 -8.18897666e-01 -4.17413472e-01  3.36187442e-01\n",
      "   1.30583334e-16]\n",
      " [ 2.92793015e+00  4.96249203e-01 -1.39731339e-01 -2.36337727e-01\n",
      "   1.30583334e-16]\n",
      " [-2.50097740e+00 -4.44979177e-01 -3.13053991e-01 -2.66371429e-01\n",
      "   1.30583334e-16]]\n",
      "\n",
      "Explained Variance Ratio:\n",
      "[7.43376469e-01 2.07069548e-01 3.76177075e-02 1.19362753e-02\n",
      " 3.80625158e-33]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24b3d09-60f9-499d-bd51-ad140f4d23c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
